

# Streaming Connectors 流连接器

## Predefined Sources and Sinks 预定义的源和接收器

A few basic data sources and sinks are built into Flink and are always available. The [predefined data sources](//ci.apache.org/projects/flink/flink-docs-release-1.7/dev/datastream_api.html#data-sources) include reading from files, directories, and sockets, and ingesting data from collections and iterators. The [predefined data sinks](//ci.apache.org/projects/flink/flink-docs-release-1.7/dev/datastream_api.html#data-sinks) support writing to files, to stdout and stderr, and to sockets.
Flink内置了一些基本数据源和接收器，它们始终可用。该预定义的数据源包括文件，目录和插座读书，并从摄取集合和迭代器的数据。该预定义的数据接收器支持写入文件，以输出和错误，并插槽。

## Bundled Connectors 捆绑式连接器

Connectors provide code for interfacing with various third-party systems. Currently these systems are supported:
连接器提供用于与各种第三方系统接口的代码。当前支持以下系统：

*   [Apache Kafka](kafka.html) (source/sink)
*   [Apache Cassandra](cassandra.html) (sink)
*   [Amazon Kinesis Streams](kinesis.html) (source/sink)
*   [Elasticsearch](elasticsearch.html) (sink)
*   [Hadoop FileSystem](filesystem_sink.html) (sink)
*   [RabbitMQ](rabbitmq.html) (source/sink)
*   [Apache NiFi](nifi.html) (source/sink)
*   [Twitter Streaming API](twitter.html) (source)

Keep in mind that to use one of these connectors in an application, additional third party components are usually required, e.g. servers for the data stores or message queues. Note also that while the streaming connectors listed in this section are part of the Flink project and are included in source releases, they are not included in the binary distributions. Further instructions can be found in the corresponding subsections.
请记住，要在应用程序中使用这些连接器之一，通常需要其他第三方组件，例如，用于数据存储或消息队列的服务器。还请注意，尽管本节中列出的流连接器是Flink项目的一部分，并包含在源代码版本中，但它们不包含在二进制发行版中。在相应的小节中可以找到更多说明。

## Connectors in Apache Bahir Apache Bahir中的连接器

Additional streaming connectors for Flink are being released through [Apache Bahir](https://bahir.apache.org/), including:
通过Apache Bahir发布了用于Flink的其他流连接器，包括：

*   [Apache ActiveMQ](https://bahir.apache.org/docs/flink/current/flink-streaming-activemq/) (source/sink)
*   [Apache Flume](https://bahir.apache.org/docs/flink/current/flink-streaming-flume/) (sink)
*   [Redis](https://bahir.apache.org/docs/flink/current/flink-streaming-redis/) (sink)
*   [Akka](https://bahir.apache.org/docs/flink/current/flink-streaming-akka/) (sink)
*   [Netty](https://bahir.apache.org/docs/flink/current/flink-streaming-netty/) (source)

## Other Ways to Connect to Flink 连接到Flink的其他方式

### Data Enrichment via Async I/O 通过异步I / O进行数据丰富

Using a connector isn’t the only way to get data in and out of Flink. One common pattern is to query an external database or web service in a `Map` or `FlatMap` in order to enrich the primary datastream. Flink offers an API for [Asynchronous I/O](//ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/operators/asyncio.html) to make it easier to do this kind of enrichment efficiently and robustly.
使用连接器并不是将数据传入和传出Flink的唯一方法。一个常见的模式是查询外部数据库或Web服务在一个Map或FlatMap以丰富的主要数据流。Flink提供了一个用于异步I / O的API，以使其更容易有效且强大地完成这种扩充。

### Queryable State 可查询状态

When a Flink application pushes a lot of data to an external data store, this can become an I/O bottleneck. If the data involved has many fewer reads than writes, a better approach can be for an external application to pull from Flink the data it needs. The [Queryable State](//ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/state/queryable_state.html) interface enables this by allowing the state being managed by Flink to be queried on demand.
当Flink应用程序将大量数据推送到外部数据存储时，这可能成为I / O瓶颈。如果涉及的数据的读取次数少于写入的次数，则更好的方法是让外部应用程序从Flink中提取所需的数据。在可查询的状态界面，允许通过弗林克管理的状态下进行按需查询支持这个。
