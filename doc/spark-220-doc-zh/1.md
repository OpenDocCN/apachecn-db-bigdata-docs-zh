# Spark 概述

Apache Spark 是一个快速的，通用的集群计算系统。它对 Java，Scala，Python 和 R 提供了的高层 API，并有一个经优化的支持通用执行图计算的引擎。它还支持一组丰富的高级工具，包括用于 SQL 和结构化数据处理的 [Spark SQL](sql-programming-guide.html)，用于机器学习的 [MLlib](ml-guide.html)，用于图计算的 [GraphX](graphx-programming-guide.html) 和 [Spark Streaming](streaming-programming-guide.html)。

# 安全

默认情况下，Spark中的安全性处于关闭状态。这意味着您默认情况下容易受到攻击。在下载和运行Spark之前，请参阅[Spark Security](https://spark.apache.org/docs/latest/security.html)。

# 下载

从该项目官网的 [下载页面](http://spark.apache.org/downloads.html) 获取 Spark。该文档用于 Spark 2.4.4 版本。Spark 可以通过 Hadoop client 库使用 HDFS 和 YARN。下载一个预编译主流 Hadoop 版本比较麻烦。用户可以下载一个编译好的 Hadoop 版本，并且可以通过[设置 Spark 的 classpath](hadoop-provided.html) 来与任何的 Hadoop 版本一起运行 Spark。Scala 和 Java 用户可以在他们的工程中通过 Maven 的方式引入 Spark，并且在将来 Python 用户也可以从 PyPI 中安装 Spark。

如果您希望从源码中编译一个Spark，请访问 [编译 Spark](building-spark.html)。

Spark 可以在 Windows 和类 UNIX 系统（例如，Linux，Mac OS）上运行。它可以很容易的在一台本地机器上运行 ——你只需要安装一个 JAVA 环境并配置 PATH 环境变量，或者让 JAVA_HOME 指向你的 JAVA 安装路径。

Spark 可运行在 Java 8，Python 2.7+/3.4+ 和 R 3.1+ 的环境上。针对 Scala API，Spark 2.4.4 使用了 Scala 2.12。您需要一个可兼容的 Scala 版本（2.12.x）。

请注意，从 Spark 2.2.0 起，对 Java 7，Python 2.6 和旧的 Hadoop 2.6.5 之前版本的支持均已被删除。

请注意，Scala 2.10 的支持已经在 Spark 2.3.0 中删除。Scala 2.11 的支持已经不再适用于 Spark 2.4.1，并将会在 Spark 3.0 中删除。

# 运行示例和 Shell

Spark 自带了几个示例程序。Scala，Java，Python 和 R 示例在 `examples/src/main` 目录中。要运行 Java 或 Scala 中的某个示例程序，在最顶层的 Spark 目录中使用 `bin/run-example &lt;class&gt; [params]` 命令即可。（这个命令底层调用了 [`spark-submit` 脚本](submitting-applications.html)去加载应用程序）。例如，

```
./bin/run-example SparkPi 10 
```

您也可以通过一个改进版的 Scala shell 来运行交互式的 Spark。这是一个来学习该框架比较好的方式。

```
./bin/spark-shell --master local[2] 
```

该 `--master` 选项指定了 [分布式集群的 master URL](submitting-applications.html#master-urls)，或者指定以 `local` 模式使用 1 个线程在本地运行，`local[N]` 会使用 N 个线程在本地运行。你应该先使用 `local` 模式进行测试。可以通过 `--help` 选项来获取 ` spark-shell`  的所有配置项。

Spark 同样支持 Python API。在 Python interpreter（解释器）中运行交互式的 Spark，请使用 `bin/pyspark`:

```
./bin/pyspark --master local[2] 
```

Spark 中也提供了 Python 应用示例。例如，

```
./bin/spark-submit examples/src/main/python/pi.py 10 
```

从 1.4 开始（仅包含了 DataFrames APIs）Spark 也提供了一个实验性的 [R API](sparkr.html)。为了在 R interpreter（解释器）中运行交互式的 Spark，请执行 `bin/sparkR`:

```
./bin/sparkR --master local[2] 
```

R 中也提供了应用示例。例如，

```
./bin/spark-submit examples/src/main/r/dataframe.R 
```

# 在集群上运行

该 Spark [集群模式概述](cluster-overview.html) 说明了在集群上运行的主要的概念。Spark 既可以独立运行，也可以在一些现有的 Cluster Manager（集群管理器）上运行。它当前提供了几种用于部署的选项:

* [Standalone Deploy Mode](spark-standalone.html)：在私有集群上部署 Spark 最简单的方式

* [Apache Mesos](running-on-mesos.html)

* [Hadoop YARN](running-on-yarn.html)

* [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html)

# 进一步学习链接

**编程指南:**

- [快速入门](https://spark.apache.org/docs/latest/quick-start.html): 对Spark API的快速介绍；从这里开始！ 
- [RDD 编程指南](https://spark.apache.org/docs/latest/rdd-programming-guide.html): Spark基础知识概述——RDD（核心但旧的API），累加器和广播变量
- [Spark SQL, Datasets, 和 DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html): 使用关系查询（比RDD更新的API）处理结构化数据
- [Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html): 使用关系查询处理结构化的数据流（使用数据集和数据帧，比DStreams更新的API）
- [Spark Streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html): 使用DStreams处理数据流（旧API）
- [MLlib](https://spark.apache.org/docs/latest/ml-guide.html): 运用机器学习算法
- [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html): 处理图

**API 文档:**

- [Spark Scala API (Scaladoc)](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)
- [Spark Java API (Javadoc)](https://spark.apache.org/docs/latest/api/java/index.html)
- [Spark Python API (Sphinx)](https://spark.apache.org/docs/latest/api/python/index.html)
- [Spark R API (Roxygen2)](https://spark.apache.org/docs/latest/api/R/index.html)
- [Spark SQL, Built-in Functions (MkDocs)](https://spark.apache.org/docs/latest/api/sql/index.html)

**部署指南：**

- [集群概述](https://spark.apache.org/docs/latest/cluster-overview.html)：在集群上运行时的概念和组件概述
- [Submitting Applications](https://spark.apache.org/docs/latest/submitting-applications.html): packaging and deploying applications
- [提交应用](https://spark.apache.org/docs/latest/submitting-applications.html)：打包和部署应用程序
- 部署模式：
  - [Amazon EC2](https://github.com/amplab/spark-ec2): 可让您在5分钟左右的时间内在EC2上启动集群的脚本
  - [Standalone 部署模式](https://spark.apache.org/docs/latest/spark-standalone.html): 此模式下无需第三方集群管理器即可快速启动独立集群
  - [Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html): 使用  [Apache Mesos](https://mesos.apache.org/) 部署私有集群
  - [YARN](https://spark.apache.org/docs/latest/running-on-yarn.html): 在Hadoop NextGen（YARN）之上部署Spark
  - [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html): 在Kubernetes上部署Spark 
