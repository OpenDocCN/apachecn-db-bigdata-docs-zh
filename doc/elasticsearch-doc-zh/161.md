# Sampler Aggregation

原文链接 : [https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-sampler-aggregation.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-sampler-aggregation.html)

译文链接 : [Sampler Aggregation](http://apache.wiki/display/Elasticsearch/Sampler+Aggregation)

贡献者 : @于永超，[ApacheCN](/display/~apachecn)，[Apache中文网](/display/~apachechina)

## Sampler Aggregation

此功能是实验性的，可能会在将来的版本中完全更改或删除。 Elastic将采取最大努力来解决任何问题，但是实验性的功能不受官方GA功能的支持

用于将任何子聚合处理限制为最高评分文档样本的过滤聚合。

## 示例用例

*   将分析的重点放在高度相关性匹配上，而不是潜在的非常长的低质量的长尾词
*   减少聚合的运行成本，只使用样本就能产生有用的结果

      e.g. `significant_terms`

例子：

一个关于StackOverflow数据的查询，用于流行的术语javascript或者更罕见的术语kibana将匹配许多文档－－他们中的大多数缺少Kibana这个词。要将significant_terms聚合重点放在最有可能与查询中最有趣的部分相匹配的最高评分文档上，我们使用如下示例

```
POST /stackoverflow/_search?size=0
{
    "query": {
        "query_string": {
            "query": "tags:kibana OR tags:javascript"
        }
    },
    "aggs": {
        "sample": {
            "sampler": {
                "shard_size": 200
            },
            "aggs": {
                "keywords": {
                    "significant_terms": {
                        "field": "tags",
                        "exclude": ["kibana", "javascript"]
                    }
                }
            }
        }
    }
}
```

响应：

```
{
    ...
    "aggregations": {
        "sample": {
            "doc_count": 1000,  ＃1
            "keywords": {
                "doc_count": 1000,
                "buckets": [
                    {
                        "key": "elasticsearch",
                        "doc_count": 150,
                        "score": 1.078125,
                        "bg_count": 200
                    },
                    {
                        "key": "logstash",
                        "doc_count": 50,
                        "score": 0.5625,
                        "bg_count": 50
                    }
                ]
            }
        }
    }
}
```

＃1   共有1000份文件被抽样，因为我们从5个碎片的索引中要求最多200个。 因此，执行嵌套的significant_terms聚合的成本是有限的，而不是无限制的。

如果没有sampler聚合，请求查询就会考虑到低质量匹配的完整“长尾”，因此不确定诸如jquery和angular等重要术语，而不是关注更有洞察力的kibana相关术语。

```
POST /stackoverflow/_search?size=0
{
    "query": {
        "query_string": {
            "query": "tags:kibana OR tags:javascript"
        }
    },
    "aggs": {
             "low_quality_keywords": {
                "significant_terms": {
                    "field": "tags",
                    "size": 3,
                    "exclude":["kibana", "javascript"]
                }
        }
    }
}
```

 响应：

```
{
    ...
    "aggregations": {
        "low_quality_keywords": {
            "doc_count": 1000,
            "buckets": [
                {
                    "key": "angular",
                    "doc_count": 200,
                    "score": 0.02777,
                   "bg_count": 200
                },
                {
                    "key": "jquery",
                    "doc_count": 200,
                    "score": 0.02777,
                    "bg_count": 200
                },
                {
                    "key": "logstash",
                    "doc_count": 50,
                    "score": 0.0069,
                    "bg_count": 50
                }
            ]
        }
    }
}
```

### shard_size

shard_size参数限制在每个分片上处理的样本中收集的顶级评分文档数量。 默认值为100。

### Limitations

不能嵌套在breadth_first聚合下

作为一个基于质量的过滤器，sampler聚合需要访问为每个文档生成的相关性分数。因此，它不能嵌套在具有collect_mode从默认depth_first模式切换到breadth_first的术语聚合，因为这丢弃分数。 在这种情况下，会抛出一个错误。