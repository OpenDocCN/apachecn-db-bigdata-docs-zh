# Tokenizers（分词器）

原文链接 : [https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-tokenizers.html](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-tokenizers.html)

译文链接 : [http://www.apache.wiki/pages/editpage.action?pageId=9405270](http://www.apache.wiki/pages/editpage.action?pageId=9405270)

贡献者 : [陈益雷](/display/~chenyilei)，[ApacheCN](/display/~apachecn)，[Apache中文网](/display/~apachechina)

一个 **tokenizer**（分词器）接收一个字符流，将之分割为独立的 **tokens**（词元，通常是独立的单词），然后输出 **tokens** 流。

例如，[whitespace](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-whitespace-tokenizer.html) **tokenizer** 遇到空白字符时分割文本。它会将文本 "**Quick brown fox!**" 分割为 [**Quick**, **brown**, **fox!**]。

该 **tokenizer**（分词器）还负责记录各个 **term**（词条）的顺序或 **position** 位置（用于 **phrase **短语和 **word** **proximity **词近邻查询），以及 **term**（词条）所代表的原始 **word**（单词）的 **start**（起始）和 **end**（结束）的 **character offsets**（字符偏移量）（用于高亮显示搜索的内容）。

**Elasticsearch** 提供了很多内置的分词器，可以用来构建 [custom analyzers](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-custom-analyzer.html "Custom Analyzer")（自定义分词器）。

### Word Oriented Tokenizers（整词分词器）

下列的分词器通常是将整个文本分为独立的单词 : 

[Standard Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-standard-tokenizer.html) （标准分词器）

**standard tokenizer** 根据 **Unicode** 文本分割算法，以单词边界分割文本。它删除大多数标点符号。 它是大多数语言的最佳选择。

[Letter Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-letter-tokenizer.html)

**letter tokenizer** 遇到非字母时分割文本。

[Lowercase Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-lowercase-tokenizer.html)（小写分词器）

**lowercase tokenizer** 类似 **letter tokenizer**，遇到非字母时分割文本，同时会将所有分割后的词元转为小写。

[Whitespace Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-whitespace-tokenizer.html)

**whitespace tokenizer** 遇到空白字符时分割位文本。

[UAX URL Email Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-uaxurlemail-tokenizer.html)

**uax_url_email tokenizer** 类似 **standard tokenizer**，只不过它会把 **URL** 和 **email **地址当成一个词元。

[Classic Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-classic-tokenizer.html)

**classic tokenizer** 是一个基于英语语法的分词器。

[Thai Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-thai-tokenizer.html)（泰语分词器）

**thai tokenizer** 将泰文文本分成单词。

### Partial Word Tokenizers（局部单词分词器）

这些分词器将文本或者单词分割为小片段，用于 **partial** **word**（局部单词）的匹配 : 

[N-Gram Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-ngram-tokenizer.html)

**ngram tokenizer** 遇到指定的字符（如 : 空白、标点）时分割文本，然后返回各个单词的 **n-grams**（连续字符的滑动窗口）。例如  **quick **→ [**qu**, **ui**, **ic**, **ck**]。

[Edge N-Gram Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-edgengram-tokenizer.html)

**edge_ngram tokenizer **遇到指定的字符（如 : 空白、标点）时分割文本，然后它返回锚定到单词开头的 **n-gram**。例如   **quick **→ [**q**, **qu**, **qui**, **quic**, **quick**]。

### Structured Text Tokenizers（结构化文本分词器）

下列的分词器通常用于结构化文本，如 身份证、邮箱、邮政编码、文件路径 : 

[Keyword Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-keyword-tokenizer.html)

**keyword tokenizer** 什么都没做，它将整个文本当成一个 词元。一般和 **token** 过滤器（例如  [lowercase](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-lowercase-tokenfilter.html)）一起使用，规范分词后的词元。

[Pattern Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-pattern-tokenizer.html)（正则分词器）

**pattern tokenizer** 使用正则表达式，在遇到单词分隔符时分割文本，或者将捕获到的匹配文本当成一个词元。

[Path Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-pathhierarchy-tokenizer.html)（路径层次分词器）

**path_hierarchy tokenizer** 把分层的值看成是文件路径，用路径分隔符分割文本，输出树上的各个节点。例如  **/foo/bar/baz** → [**/foo**, **/foo/bar**, **/foo/bar/baz**]。