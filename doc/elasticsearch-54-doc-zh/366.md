# Compound Word Token Filter（复合词过滤器）

原文链接 :[https://www.elastic.co/guide/en/elasticsearch/reference/5.4/analysis-compound-word-tokenfilter.html](https://www.elastic.co/guide/en/elasticsearch/reference/5.4/analysis-compound-word-tokenfilter.html)

译文链接 : [http://www.apache.wiki/pages/viewpage.action?pageId=10027907](http://www.apache.wiki/pages/viewpage.action?pageId=10027907)

贡献者 : [李亚运](/display/~liyayun)，[ApacheCN](/display/~apachecn)，[Apache中文网](/display/~apachechina)

## 简述

`hyphenation_decompounder`和`dictionary_decompounder`过滤器可以将许多德语中的复合词进行拆分。

两个过滤器都需要单词字典，可以按如下方式提供：

| `word_list` | 或一系列字，在令牌过滤器配置中内联指定 |
| `word_list_path` | UTF-8编码文件的路径（绝对或相对于`config`目录），每行包含一个字。 |

## Hyphenation decompounder（连词分解）

`hyphenation_decompounder`使用连字符语法来查找潜在的字词，然后对单词字典进行检查。 输出的token质量与您使用的语法文件的质量直接相关。 对于像德语这样的语言是非常适用的。 

基于XML的连字符语法文件可以在“ [对象格式化对象](http://offo.sourceforge.net/#FOP+XML+Hyphenation+Patterns) （OFFO）Sourceforge”项目中找到。 目前仅支持FOP v1.2兼容连字符文件。 您可以直接下载[offo-hyphenation_v1.2.zip](https://sourceforge.net/projects/offo/files/offo-hyphenation/1.2/offo-hyphenation_v1.2.zip/downloadALkJrhh1HOK03GnbuWiuoThBDDDion7pCg)并查看`offo-hyphenation/hyph/`目录。想了解更多可以去查看Apache FOP项目。

## Dictionary decompounder（字典分解）

`dictionary_decompounder`使用强力方法与仅字典字典结合使用复合词中的子词。 它比连字符分解器慢得多，但可以作为检验字典质量的第一步。

## Compound token filter parameters（复合词元过滤器参数）

以下参数可用于配置复合词元过滤器：

| `type` | 任何一个`dictionary_decompounder`或者是`hyphenation_decompounder` 。 |
| `word_list` | 包含用于单词字典的单词列表的数组。 |
| `word_list_path` | 单词字典的路径（绝对或相对于`config`目录）。 |
| `hyphenation_patterns_path` | FOP XML连字符模式文件的路径（绝对或相对于`config`目录）。（当连词分解时需要配置） |
| `min_word_size` | 最小字大小。 默认为5。 |
| `min_subword_size` | 最小子字大小。 默认为2。 |
| `max_subword_size` | 最大字大小。 默认为15。 |
| `only_longest_match` | 是否只包括最长的匹配子字。 默认为`false` |

如下例所示：

```
index :
    analysis :
        analyzer :
            myAnalyzer2 :
                type : custom
                tokenizer : standard
                filter : [myTokenFilter1, myTokenFilter2]
        filter :
            myTokenFilter1 :
                type : dictionary_decompounder
                word_list: [one, two, three]
            myTokenFilter2 :
                type : hyphenation_decompounder
                word_list_path: path/to/words.txt
                hyphenation_patterns_path: path/to/fop.xml
                max_subword_size : 22
```