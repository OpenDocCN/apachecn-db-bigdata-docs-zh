# UAX URL Email Tokenizer

**uax_url_email tokenizer** 类似 **standard tokenizer**，只不过它会把 **URL** 和 **email **地址当成一个词元。

原文链接 : [https://www.elastic.co/guide/en/elasticsearch/reference/5.4/analysis-uaxurlemail-tokenizer.html](https://www.elastic.co/guide/en/elasticsearch/reference/5.4/analysis-uaxurlemail-tokenizer.html)

译文链接 : [http://www.apache.wiki/pages/editpage.action?pageId=10028865](http://www.apache.wiki/pages/editpage.action?pageId=10028865)

贡献者 : [陈益雷](/display/~chenyilei)，[ApacheCN](/display/~apachecn)，[Apache中文网](/display/~apachechina)

## **输出示例**

```
POST _analyze
{
  "tokenizer": "uax_url_email",
  "text": "Email me at john.smith@global-international.com"
}
```

上面的句子会生成如下的词元:

```
[ Email, me, at, john.smith@global-international.com ]
```

而 `standard` tokenizer 会生成：

```
[ Email, me, at, john.smith, global, international.com ]
```

## **配置**

 **`uax_url_email` tokenizer** 接受以下参数：

_**max_token_length**_   单个 token 的最大长度。如果一个 token 超过这个长度，则以  `max_token_length 为间隔分割。默认值是` `255`.。

## **配置示例**

下面的例子中，我们配置标准分词器的  `max_token_length 为 5 (便于展示)：`

```
PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "uax_url_email",
          "max_token_length": 5
        }
      }
    }
  }
}

POST my_index/_analyze
{
  "analyzer": "my_analyzer",
  "text": "john.smith@global-international.com"
}
```

输出如下：

```
[ john, smith, globa, l, inter, natio, nal.c, om ]
```